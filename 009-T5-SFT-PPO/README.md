# OpposiText - Transformer Model to generate text with the "opposite" meaning.

The model generates sentences with an inverted / negated / opposite meaning of
it's input sentence. Overall, the project focuses on sentences of up to 12
words.

OpposiText is a simple transformer model based on the T5 architecture. The
model is bootstrapped from the T5-small baseline using the following
techniques:

1. Supervised fine-tuning
2. Reward modeling
3. RLHF using PPO
4. Training data augmentation

I evaluate the models using ELO ratings to show how the techniques improve the
performance of the model.

# Definition of the Task

For an input sentence, return a sentence with the opposite meaning. For a
simple sentence like "X is Y" it should be "X is not Y" or "X is Z" where Z is
an antonym.

For more complex sentences like "X is Y and Z" it should be "X is not Y and not
Z".

For questions, asking a different question is good enough.

X if Y -> Not X if Not Y.

Do X to achieve Y.
Do not do X to achieve not Y.


# Base model: T5-simple
The base model used for this task is the T5 simple. [ADD DESCRIPTION]

# Supervised Fine-Tuning

I do several iterations of getting training data.

## Bootstrapping: A list of antonyms.
I collect a list of 400 antonyms. I use the fact that "A = NOT(B)" also implies
that "B = NOT(A)" so we can double the training data.

## Simple sentences.

## Mining opposites.
I implement sft_viewer.py. I run the model against a corpus of prompts. I then
get the outputs. I pre-filter the outputs and then manually select what's OK.
This is added to the SFT examples.

# RLHF

## Reward Modeling

### Rule-Based

Part 1: $\alpha$:
* Don't produce identical outputs.
* The length should be similar
* Punctuation should be similar

Part 2: $\beta$:
* Not all the words should change (jaccard index over words)
* No excessive reperition of words

This overall rule reward is then $\alpha \cdot \beta$.

### Preference Data
I implement ppo-viewer. Present 2 alternatives, and I pick.

Overall, the reward model didn't really learn that e.g. the identity is to be
avoided.

I did two things:
1) I generated a list of preference pairs from the decoding on a prompt corpus.
Base is the identity (if it was generated by the model) and Winner is the best
output as proposed by the model.

2) I mix the rule-based reward model with the learned reward model.

