OpposiText - Transformer Model to generate sentences with the "opposite" meaning.

OpposiText is a simple transformer model based on the T5 architecture. CITATION.

In this project I apply common techniques to train transformer models:
* Supervised fine-tuning
* Reward modeling
* RLHF using PPO
* Training data augmentation

I evaluate the models using ELO ratings to show how the techniques improve the
performance of the model.

# Definition of the Task


# Base model: T5-simple
The base model used for this task is the T5 simple. [ADD DESCRIPTION]

# Supervised Fine-Tuning

I do several iterations of getting training data.

## Bootstrapping: A list of antonyms.
I collect a list of 400 antonyms. I use the fact that "A = NOT(B)" also implies
that "B = NOT(A)" so we can double the training data.

## Simple sentences.

## Mining opposites.
I implement sft_viewer.py. I run the model against a corpus of prompts. I then
get the outputs. I pre-filter the outputs and then manually select what's OK.
This is added to the SFT examples.

# RLHF

## Reward Modeling

### Rule-Based

Part 1: $\alpha$:
* Don't produce identical outputs.
* The length should be similar
* Punctuation should be similar

Part 2: $\beta$:
* Not all the words should change (jaccard index over words)
* No excessive reperition of words

This overall rule reward is then $\alpha \cdot \beta$.

### Preference Data
I implement ppo-viewer. Present 2 alternatives, and I pick.

Overall, the reward model didn't really learn that e.g. the identity is to be
avoided.

I did two things:
1) I generated a list of preference pairs from the decoding on a prompt corpus.
Base is the identity (if it was generated by the model) and Winner is the best
output as proposed by the model.

2) I mix the rule-based reward model with the learned reward model.

